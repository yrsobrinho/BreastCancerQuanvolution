{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "from medmnist import BreastMNIST, INFO\n",
    "import pennylane as qml\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as FS\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = INFO['breastmnist']\n",
    "data_flag = 'breastmnist'\n",
    "DataClass = BreastMNIST\n",
    "\n",
    "task = info['task']  \n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "print(f\"Number of classes: {n_classes}\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  \n",
    "    transforms.RandomRotation(degrees=15),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "data_train = DataClass(split='train', transform=train_transform, download=True)\n",
    "data_test = DataClass(split='test', transform=eval_transform, download=True)\n",
    "data_eval = DataClass(split='val', transform=eval_transform, download=True)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "dataloader_train = data.DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = data.DataLoader(dataset=data_test, batch_size=batch_size, shuffle=False)\n",
    "dataloader_eval = data.DataLoader(dataset=data_eval, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quanvolution(nn.Module):\n",
    "    def __init__(self, stride=2, device=\"lightning.gpu\", wires=4, circuit_layers=1, n_rotations=8, out_channels=4, seed=None):\n",
    "        super(Quanvolution, self).__init__()\n",
    "        \n",
    "        self.wires = wires\n",
    "        self.dev = qml.device(device, wires=self.wires)\n",
    "        \n",
    "        self.stride = stride\n",
    "        self.out_channels = min(out_channels, wires)\n",
    "        \n",
    "        if seed is None:\n",
    "            seed = np.random.randint(low=0, high=10e6)\n",
    "        \n",
    "        @qml.qnode(device=self.dev, interface=\"torch\")\n",
    "        def circuit(inputs, weights):\n",
    "            n_inputs=4\n",
    "            for j in range(n_inputs):\n",
    "                qml.RY(inputs[j], wires=j)\n",
    "            qml.templates.layers.RandomLayers(weights, wires=list(range(self.wires)), seed=seed)\n",
    "            \n",
    "            return [qml.expval(qml.PauliZ(j)) for j in range(self.out_channels)]\n",
    "        \n",
    "        weight_shapes = {\"weights\": [circuit_layers, n_rotations]}\n",
    "        self.circuit = qml.qnn.TorchLayer(circuit, weight_shapes=weight_shapes)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        bs, h, w, ch = img.size()\n",
    "        if ch > 1:\n",
    "            img = img.mean(axis=-1).reshape(bs, h, w, 1)\n",
    "                        \n",
    "        kernel_size = 2        \n",
    "        h_out = (h-kernel_size) // self.stride + 1\n",
    "        w_out = (w-kernel_size) // self.stride + 1\n",
    "        \n",
    "        \n",
    "        out = torch.zeros((bs, h_out, w_out, self.out_channels))\n",
    "        \n",
    "        for b in range(bs):\n",
    "            for j in range(0, h_out, self.stride):\n",
    "                for k in range(0, w_out, self.stride):\n",
    "                    q_results = self.circuit(\n",
    "                        inputs=torch.Tensor([\n",
    "                            img[b, j, k, 0],\n",
    "                            img[b, j, k + 1, 0],\n",
    "                            img[b, j + 1, k, 0],\n",
    "                            img[b, j + 1, k + 1, 0]\n",
    "                        ])\n",
    "                    )\n",
    "                    for c in range(self.out_channels):\n",
    "                        out[b, j // kernel_size, k // kernel_size, c] = q_results[c]        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quanvolution4x1Model(nn.Module):\n",
    "    def __init__(self, output_size=(14, 14), patch_size=2, n_qubits=4, num_classes=2):\n",
    "        super(Quanvolution4x1Model, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.quanv = Quanvolution()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(output_size[0] * output_size[1] * n_qubits, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:  \n",
    "            x = x.unsqueeze(1) \n",
    "\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.quanv(x).to(device)\n",
    "        x = torch.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Quanvolution4x1Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1) \n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_model_path = \"/home/eflammere/BreastCancerQuanvolution/Quantum/checkpoints/trainable/BreastMNIST/28x28/1/last_model.pth\"\n",
    "checkpoint_frequency = 2\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1_scores = []\n",
    "val_aucs = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(\"\\n[Training]\")\n",
    "    for batch_idx, (images, labels) in enumerate(tqdm(dataloader_train, desc=\"Training Batches\", bar_format=\"{desc}: {n}/{total}\")):\n",
    "        images, labels = images.squeeze(1).to(device), labels.squeeze().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_accuracy = accuracy_score(\n",
    "            labels.cpu().numpy(), output.argmax(dim=1).cpu().numpy()\n",
    "        )\n",
    "\n",
    "        print(f\"Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.3f}\")\n",
    "\n",
    "        if hasattr(model, 'quanv') and hasattr(model.quanv, 'circuit'):\n",
    "            print(\"\\nGradients Layer 0:\")\n",
    "            print(model.quanv.circuit.weights.grad)\n",
    "\n",
    "    epoch_train_loss = total_loss / len(dataloader_train)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    print(f\"Epoch {epoch + 1} Training Loss: {epoch_train_loss:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_labels, val_predictions = [], []\n",
    "\n",
    "    print(\"\\n[Validation]\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(dataloader_eval, desc=\"Validation Batches\", bar_format=\"{desc}: {n}/{total}\")):\n",
    "            images, labels = images.squeeze(1).to(device), labels.squeeze().to(device)\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_labels.append(labels)\n",
    "            val_predictions.append(output)\n",
    "\n",
    "            batch_accuracy = accuracy_score(\n",
    "                labels.cpu().numpy(), output.argmax(dim=1).cpu().numpy()\n",
    "            )\n",
    "            print(f\"Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.3f}\")\n",
    "\n",
    "    epoch_val_loss = val_loss / len(dataloader_eval)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_labels = torch.cat(val_labels)\n",
    "    val_predictions = torch.cat(val_predictions)\n",
    "\n",
    "    val_accuracy = accuracy_score(\n",
    "        val_labels.cpu().numpy(), val_predictions.argmax(dim=1).cpu().numpy())\n",
    "    val_precision = precision_score(\n",
    "        val_labels.cpu().numpy(), val_predictions.argmax(dim=1).cpu().numpy(),\n",
    "        average=\"weighted\", zero_division=0)\n",
    "    val_recall = recall_score(\n",
    "        val_labels.cpu().numpy(), val_predictions.argmax(dim=1).cpu().numpy(),\n",
    "        average=\"weighted\", zero_division=0)\n",
    "    val_f1 = f1_score(\n",
    "        val_labels.cpu().numpy(), val_predictions.argmax(dim=1).cpu().numpy(),\n",
    "        average=\"weighted\", zero_division=0)\n",
    "    val_auc = roc_auc_score(\n",
    "        val_labels.cpu().numpy(), val_predictions[:, 1].cpu().numpy())\n",
    "\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_precisions.append(val_precision)\n",
    "    val_recalls.append(val_recall)\n",
    "    val_f1_scores.append(val_f1)\n",
    "    val_aucs.append(val_auc)\n",
    "\n",
    "    print(\n",
    "        f\"\\nEpoch {epoch + 1} Summary:\\n\"\n",
    "        f\"Train Loss: {epoch_train_loss:.4f}, \"\n",
    "        f\"Val Loss: {epoch_val_loss:.4f}, \"\n",
    "        f\"Accuracy: {val_accuracy:.3f}, \"\n",
    "        f\"Precision: {val_precision:.3f}, \"\n",
    "        f\"Recall: {val_recall:.3f}, \"\n",
    "        f\"F1: {val_f1:.3f}, \"\n",
    "        f\"AUC: {val_auc:.3f}\"\n",
    "    )\n",
    "\n",
    "    if (epoch + 1) % checkpoint_frequency == 0:\n",
    "        checkpoint_path = f\"/home/eflammere/BreastCancerQuanvolution/Quantum/checkpoints/trainable/BreastMNIST/28x28/1/model_checkpoint_epoch_{epoch + 1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved.\")\n",
    "\n",
    "torch.save(model.state_dict(), last_model_path)\n",
    "print(\"Last model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"/home/eflammere/BreastCancerQuanvolution/Quantum/checkpoints/BCDR/model_checkpoint_epoch_20.pth\"\n",
    "# model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "test_loss = 0.0\n",
    "test_labels, test_predictions = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader_test:\n",
    "        images, labels = images.squeeze(1).to(device), labels.squeeze().to(device)\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        test_loss += loss.item()\n",
    "        test_labels.append(labels)\n",
    "        test_predictions.append(output)\n",
    "\n",
    "test_labels = torch.cat(test_labels)\n",
    "test_predictions = torch.cat(test_predictions)\n",
    "\n",
    "test_accuracy = accuracy_score(\n",
    "    test_labels.cpu().numpy(), test_predictions.argmax(dim=1).cpu().numpy()\n",
    ")\n",
    "test_precision = precision_score(\n",
    "    test_labels.cpu().numpy(), test_predictions.argmax(dim=1).cpu().numpy(), \n",
    "    average=\"weighted\", zero_division=0\n",
    ")\n",
    "test_recall = recall_score(\n",
    "    test_labels.cpu().numpy(), test_predictions.argmax(dim=1).cpu().numpy(), \n",
    "    average=\"weighted\", zero_division=0\n",
    ")\n",
    "test_f1 = f1_score(\n",
    "    test_labels.cpu().numpy(), test_predictions.argmax(dim=1).cpu().numpy(), \n",
    "    average=\"weighted\", zero_division=0\n",
    ")\n",
    "test_auc = roc_auc_score(\n",
    "    test_labels.cpu().numpy(), test_predictions[:, 1].cpu().numpy()\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Test Evaluation:\")\n",
    "print(f\"Test Loss: {test_loss / len(test_loader):.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(\n",
    "    test_labels.cpu().numpy(), test_probs[:, 1].cpu().numpy()\n",
    ")\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(false_positive_rate, true_positive_rate, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--') \n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "dataset_name = \"BreastMNIST\"\n",
    "roc_data = pd.DataFrame({\n",
    "    'Dataset': [dataset_name] * len(false_positive_rate),\n",
    "    'False Positive Rate': false_positive_rate,\n",
    "    'True Positive Rate': true_positive_rate,\n",
    "    'Thresholds': thresholds\n",
    "})\n",
    "roc_data.to_csv(f'/home/eflammere/BreastCancerQuanvolution/Quantum/checkpoints/trainable/BreastMNIST/28x28/1/roc_curve_data_{dataset_name}.csv', index=False)\n",
    "\n",
    "print(f\"ROC curve data exported to 'roc_curve_data_{dataset_name}.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels.cpu().numpy(), test_predictions.argmax(dim=1).cpu().numpy(), labels=[0, 1])\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantumEnvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
